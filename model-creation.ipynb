{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_section(x, num_filters = 64, kernel_size = (3, 3), activation = \"relu\"):\n",
    "\tconv_layer = tf.keras.layers.Conv2D(num_filters, kernel_size, activation = activation, padding = \"same\")(x)\n",
    "\tmaxpool_layer = tf.keras.layers.MaxPool2D()(conv_layer)\n",
    "\treturn conv_layer, maxpool_layer\n",
    "\n",
    "def inverse_conv_section(x, residual_layer, num_filters = 64, kernel_size = (3, 3), activation = \"relu\"):\n",
    "\tinverse_conv_layer = tf.keras.layers.Conv2DTranspose(num_filters, kernel_size, activation = activation, padding = \"same\")(x + residual_layer)\n",
    "\tupsample_layer = tf.keras.layers.UpSampling2D()(inverse_conv_layer)\n",
    "\treturn upsample_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net\n",
    "def u_net(input_shape, filter_architecture, num_sections):\n",
    "\tinp = tf.keras.layers.Input(shape = input_shape)\n",
    "\tx = inp\n",
    "\tresiduals = []\n",
    "\n",
    "\t# Downstream\n",
    "\tfor i in range(num_sections):\n",
    "\t\tresidual, x = conv_section(\n",
    "\t\t\tx,\n",
    "\t\t\tfilter_architecture[i][\"filters\"],\n",
    "\t\t\tfilter_architecture[i][\"kernel\"],\n",
    "\t\t\tfilter_architecture[i][\"activation\"])\n",
    "\t\tresiduals.append(residual)\n",
    "\t\n",
    "\tx = tf.keras.layers.Conv2D(\n",
    "\t\tfilter_architecture[num_sections][\"filters\"],\n",
    "\t\tfilter_architecture[num_sections][\"kernel\"],\n",
    "\t\tactivation = filter_architecture[num_sections][\"activation\"])(x)\n",
    "\n",
    "\t# Upstream\n",
    "\tfor i in range(num_sections - 1, -1, -1):\n",
    "\t\tx = inverse_conv_section(\n",
    "\t\t\tx,\n",
    "\t\t\tresiduals[i],\n",
    "\t\t\tfilter_architecture[i][\"filters\"],\n",
    "\t\t\tfilter_architecture[i][\"kernel\"],\n",
    "\t\t\tfilter_architecture[i][\"activation\"])\n",
    "\n",
    "\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = []\n",
    "num_sections = 4\n",
    "\n",
    "for i in range(num_sections):\n",
    "\tmodel_architecture.append({\n",
    "\t\t\"filters\": 64,\n",
    "\t\t\"kernel\": (4, 4),\n",
    "\t\t\"activation\": \"relu\"\n",
    "\t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture.append({\n",
    "\t\"filters\": 64,\n",
    "\t\"kernel\": (4, 4),\n",
    "\t\"activation\": \"relu\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = u_net(input_shape = (64, 64, 3), filter_architecture = model_architecture, num_sections = num_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unet_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-57ffd6a29fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unet_model' is not defined"
     ]
    }
   ],
   "source": [
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_masking(frames, num_elements_front_and_back = 3):\n",
    "\tX = []\n",
    "\ty = []\n",
    "\tfor i in range(num_elements_front_and_back, len(frames) - num_elements_front_and_back):\n",
    "\n",
    "\t\tprevious_frames = frames[i-num_elements_front_and_back:i]\n",
    "\n",
    "\t\t# after frames specifically ordered to go from future to present (in that direction)\n",
    "\t\tafter_frames = frames[i+num_elements_front_and_back:i:-1]\n",
    "\t\tX.append((previous_frames, after_frames))\n",
    "\t\ty.append(frames[i])\n",
    "\t\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DMHAUnit(tf.keras.layers.Layer):\n",
    "\n",
    "\tdef __init__(self,\n",
    "\tnum_heads: int,\n",
    "\td_model: int,\n",
    "\timage_size: tuple,\n",
    "\tkernel_size: Iterable,\n",
    "\tname: str,\n",
    "\tfeature_activation: str = \"relu\",\n",
    "\toutput_activation: str = \"linear\"):\n",
    "\t\tsuper().__init__(name = name)\n",
    "\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.query_size = self.d_model // self.num_heads\n",
    "\t\tself.feature_activation = feature_activation\n",
    "\t\tself.output_activation = output_activation\n",
    "\t\tself.image_size = image_size\n",
    "\t\tself.kernel_size = kernel_size\n",
    "\n",
    "\t\tassert self.d_model % self.num_heads == 0, \"D_model and Number of Heads do not match\"\n",
    "\n",
    "\t\tself.num_blocks_y = self.image_size[0] // self.kernel_size[0]\n",
    "\n",
    "\t\tself.num_blocks_x = self.image_size[1] // self.kernel_size[1]\n",
    "\n",
    "\t\t# self.y_complete = (self.image_size[0] % self.kernel_size[0] == 0)\n",
    "\n",
    "\t\t# self.x_complete = (self.image_size[1] % self.kernel_size[1] == 0)\n",
    "\n",
    "\t\tself.y_pad_length = 0\n",
    "\n",
    "\t\tself.x_pad_length = 0\n",
    "\n",
    "\t\t# if not self.y_complete:\n",
    "\n",
    "\t\t# \tself.y_pad_length = self.kernel_size[0] - (self.image_size[0] % self.kernel_size[0])\n",
    "\n",
    "\t\t# \tself.num_blocks_y += 1\n",
    "\n",
    "\t\t# if not self.x_complete:\n",
    "\n",
    "\t\t# \tself.x_pad_length = self.kernel_size[1] - (self.image_size[1] % self.kernel_size[1])\n",
    "\n",
    "\t\t# \tself.num_blocks_x += 1\n",
    "\n",
    "\t\tself.y_padded = self.image_size[0] + self.y_pad_length\n",
    "\n",
    "\t\tself.x_padded = self.image_size[1] + self.x_pad_length\n",
    "\t\n",
    "\t\tself.total_num_blocks = self.num_blocks_x * self.num_blocks_y\n",
    "\n",
    "\t\tself.q_dense, self.k_dense, self.v_dense = [\n",
    "\t\t\ttf.keras.layers.Dense(self.d_model,\n",
    "\t\t\tactivation = feature_activation,\n",
    "\t\t\tname = f\"{name}_feature_dense_{i}\") for i in range(3)]\n",
    "\n",
    "\tdef call(self, X):\n",
    "\n",
    "\t\tX_shape = tf.shape(X)\n",
    "\n",
    "\t\tb, h, w, c = X_shape[0], X_shape[1], X_shape[2], X_shape[3]\n",
    "\t\t\n",
    "\n",
    "\t\tX_reshaped = tf.reshape(X, (b, h * w, c))\n",
    "\n",
    "\t\t# features: (b, h * w, d_model)\n",
    "\n",
    "\t\tq_features = self.q_dense(X_reshaped)\n",
    "\t\tk_features = self.k_dense(X_reshaped)\n",
    "\t\tv_features = self.v_dense(X_reshaped)\n",
    "\n",
    "\t\tq_features /= (self.d_model ** .5)\n",
    "\n",
    "\t\t# split heads\n",
    "\t\tq_heads = tf.reshape(q_features, (b, self.num_heads, h, w, self.query_size))\n",
    "\t\tk_heads = tf.reshape(k_features, (b, self.num_heads, h, w, self.query_size))\n",
    "\t\tv_heads = tf.reshape(v_features, (b, self.num_heads, h, w, self.query_size))\n",
    "\n",
    "\t\t# pad to allow for kernel splits\n",
    "\t\t# if (self.x_complete and self.y_complete) is not True:\n",
    "\n",
    "\t\t# \tpadding = tf.constant([\n",
    "\t\t# \t\t[0, 0],\n",
    "\t\t# \t\t[0, 0],\n",
    "\t\t# \t\t[0, self.y_pad_length],\n",
    "\t\t# \t\t[0, self.x_pad_length],\n",
    "\t\t# \t\t[0, 0]\n",
    "\t\t# \t])\n",
    "\n",
    "\t\t# \tpadded_q_heads = tf.pad(q_heads, padding) # output: (batch_size, num_heads, h padded, w padded, query_size)\n",
    "\t\t# \tpadded_k_heads = tf.pad(k_heads, padding) # output: (batch_size, num_heads, h padded, w padded, query_size)\n",
    "\t\t# \tpadded_v_heads = tf.pad(v_heads, padding) # output: (batch_size, num_heads, h padded, w padded, query_size)\n",
    "\n",
    "\t\t# else:\n",
    "\t\t\t\n",
    "\t\tpadded_q_heads = q_heads\n",
    "\t\tpadded_k_heads = k_heads\n",
    "\t\tpadded_v_heads = v_heads\n",
    "\t\n",
    "\t\t# reshape to add kernels\n",
    "\t\tpadded_q_heads = tf.reshape(\n",
    "\t\t\tpadded_q_heads,\n",
    "\t\t\t(b,\n",
    "\t\t\tself.num_heads,\n",
    "\t\t\tself.total_num_blocks,\n",
    "\t\t\tself.query_size,\n",
    "\t\t\tself.kernel_size[0],\n",
    "\t\t\tself.kernel_size[1])\n",
    "\t\t)\n",
    "\n",
    "\t\tpadded_k_heads = tf.reshape(\n",
    "\t\t\tpadded_k_heads,\n",
    "\t\t\t(b,\n",
    "\t\t\tself.num_heads,\n",
    "\t\t\tself.total_num_blocks,\n",
    "\t\t\tself.query_size,\n",
    "\t\t\tself.kernel_size[0],\n",
    "\t\t\tself.kernel_size[1])\n",
    "\t\t)\n",
    "\n",
    "\t\tpadded_v_heads = tf.reshape(\n",
    "\t\t\tpadded_v_heads,\n",
    "\t\t\t(b,\n",
    "\t\t\tself.num_heads,\n",
    "\t\t\tself.total_num_blocks,\n",
    "\t\t\tself.query_size,\n",
    "\t\t\tself.kernel_size[0],\n",
    "\t\t\tself.kernel_size[1])\n",
    "\t\t)\n",
    "\n",
    "\t\t# create attention score\n",
    "\t\tattention = tf.einsum(\"...ijk,...njk->...injk\", padded_q_heads, padded_k_heads)\n",
    "\t\tsoftmax_attention_score = tf.math.softmax(attention)\n",
    "\n",
    "\t\t# Use einsum for matmul with different ranked tensors\n",
    "\t\t# output shape: (b, num_heads, num_blocks, query_size, kernel_size_h, kernel_size_w)\n",
    "\t\tself_attention_value_padded_unreshaped = tf.einsum(\"...injk,...njk->...ijk\", softmax_attention_score, padded_v_heads)\n",
    "\n",
    "\t\tself_attention_value_padded = tf.reshape(self_attention_value_padded_unreshaped,\n",
    "\t\t(b, self.y_padded, self.x_padded, self.d_model))\n",
    "\n",
    "\t\tself_attention_value = self_attention_value_padded# [:, :h, :w, :]\n",
    "\n",
    "\t\treturn self_attention_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_conv_lstm_attention_bottleneck_model(\n",
    "\t# overall parameters\n",
    "\td_model: int = 64,\n",
    "\tnum_unet_sections: int = 3,\n",
    "\tnum_layers_attention: int = 2,\n",
    "\n",
    "\t# conv lstm parameters\n",
    "\tconvlstm_kernel_size: tuple = (3, 3),\n",
    "\tactivation: str = \"relu\",\n",
    "\timage_dims: tuple = (64, 64, 3),\n",
    "\tseq_len_prev_and_after: int = 3,\n",
    "\n",
    "\t# attention bottleneck parameters\n",
    "\tattention_bottleneck_multiple: int = 2,\n",
    "\tnum_heads: int = 4,\n",
    "\tattention_kernel_size: tuple = (4, 4),\n",
    "\tattention_feature_activation: str = \"relu\",\n",
    "\tattention_output_activation: str = \"linear\",\n",
    "\n",
    "\t# UNet Upsample parameters\n",
    "\tunet_upsample_kernel: tuple = (3, 3),\n",
    "\tprediction_activation: str = \"tanh\",\n",
    "\t):\n",
    "\n",
    "\tresidual_tensors = []\n",
    "\n",
    "\tprev_x = tf.keras.layers.Input(shape = (seq_len_prev_and_after,) + tuple(image_dims))\n",
    "\tafter_x = tf.keras.layers.Input(shape = (seq_len_prev_and_after,) + tuple(image_dims))\n",
    "\n",
    "\tfor i in range(num_unet_sections):\t\n",
    "\t\tprev_x = tf.keras.layers.ConvLSTM2D(d_model, convlstm_kernel_size, activation = activation, return_sequences = True)(prev_x)\n",
    "\t\tafter_x = tf.keras.layers.ConvLSTM2D(d_model, convlstm_kernel_size, activation = activation, return_sequences = True)(after_x)\n",
    "\t\tcombined_x = tf.reduce_sum(tf.concat([prev_x, after_x], axis = -1), axis = -4) / tf.cast((d_model * 2) ** .5, tf.float32)\n",
    "\t\tresidual_tensors.append(combined_x)\n",
    "\t\tprev_x = tf.keras.layers.MaxPool3D((1, 2, 2))(prev_x)\n",
    "\t\tafter_x = tf.keras.layers.MaxPool3D((1, 2, 2))(after_x)\n",
    "\n",
    "\tprev_conv = tf.keras.layers.ConvLSTM2D(d_model, convlstm_kernel_size, activation = activation, return_sequences = False)(prev_x)\n",
    "\tafter_conv = tf.keras.layers.ConvLSTM2D(d_model, convlstm_kernel_size, activation = activation, return_sequences = False)(after_x)\n",
    "\n",
    "\t\"\"\"\n",
    "\t* * * a\n",
    "\t* * * *\n",
    "\tp * * *\n",
    "\n",
    "\t  |\n",
    "\t  V\n",
    "\n",
    "\t* | * | * | a\n",
    "\t* | * | * | *\n",
    "\t* | * | * | *\n",
    "\tp | * | * | *\n",
    "\t\"\"\"\n",
    "\n",
    "\tchannels = tf.shape(prev_conv)[-1]\n",
    "\n",
    "\tattention_bottleneck_block_width = tf.shape(prev_conv)[-2]\n",
    "\n",
    "\tattention_bottleneck_block_height = tf.shape(prev_conv)[-3]\n",
    "\n",
    "\tb = tf.shape(prev_conv)[0]\n",
    "\n",
    "\tvert_cols = []\n",
    "\n",
    "\tvert_strip_prev = tf.zeros((b, (attention_bottleneck_multiple + 1) * attention_bottleneck_block_height, attention_bottleneck_block_width, channels))\n",
    "\n",
    "\tvert_col_prev = tf.concat([vert_strip_prev, prev_conv], axis = -3)\n",
    "\n",
    "\tvert_cols.append(vert_col_prev)\n",
    "\n",
    "\tfor i in range(attention_bottleneck_multiple):\n",
    "\t\t\n",
    "\t\tvert_strip = tf.zeros((b, (attention_bottleneck_multiple + 2) * attention_bottleneck_block_height, attention_bottleneck_block_width, channels))\n",
    "\n",
    "\t\tvert_cols.append(vert_strip)\n",
    "\t\n",
    "\tvert_strip_after = tf.zeros((b, (attention_bottleneck_multiple + 1) * attention_bottleneck_block_height, attention_bottleneck_block_width, channels))\n",
    "\n",
    "\tvert_col_after = tf.concat([after_conv, vert_strip_after], axis = -3)\n",
    "\n",
    "\tvert_cols.append(vert_col_after)\n",
    "\n",
    "\tattention_bottleneck = tf.concat(vert_cols, axis = -2)\n",
    "\n",
    "\th = tf.shape(attention_bottleneck)[-3]\n",
    "\tw = tf.shape(attention_bottleneck)[-2]\n",
    "\n",
    "\tfor i in range(num_layers_attention):\n",
    "\n",
    "\t\tprint(tf.shape(attention_bottleneck))\n",
    "\n",
    "\t\tattention_bottleneck = Conv2DMHAUnit(\n",
    "\t\t\tnum_heads = num_heads,\n",
    "\t\t\td_model = d_model,\n",
    "\t\t\timage_size = (h, w),\n",
    "\t\t\tkernel_size = attention_kernel_size,\n",
    "\t\t\tname = f\"Conv2DMHAUnit_{i}\",\n",
    "\t\t\tfeature_activation = attention_feature_activation,\n",
    "\t\t\toutput_activation = attention_output_activation\n",
    "\t\t)(attention_bottleneck)\n",
    "\t\n",
    "\tprev_attention = attention_bottleneck[..., (attention_bottleneck_multiple + 1) * attention_bottleneck_block_height:, :attention_bottleneck_block_width, :]\n",
    "\n",
    "\tafter_attention = attention_bottleneck[..., :attention_bottleneck_block_height, (attention_bottleneck_multiple + 1) * attention_bottleneck_block_width:, :]\n",
    "\n",
    "\tconcatted_attention = tf.concat([prev_attention, after_attention], axis = -1)\n",
    "\n",
    "\tatt_upsample = concatted_attention\n",
    "\n",
    "\tfor i in range(num_unet_sections):\n",
    "\t\tatt_upsample = tf.keras.layers.UpSampling2D()(att_upsample)\n",
    "\t\tresidual_section = residual_tensors.pop()\n",
    "\t\tresidual_added_att = tf.concat([att_upsample, residual_section], axis = -1)\n",
    "\t\tatt_upsample = tf.keras.layers.Conv2D(\n",
    "\t\t\td_model,\n",
    "\t\t\tunet_upsample_kernel,\n",
    "\t\t\tactivation = activation\n",
    "\t\t\t)(residual_added_att)\n",
    "\t\n",
    "\tfinal_conv_prediction = tf.keras.layers.Conv2D(3, unet_upsample_kernel, activation = prediction_activation)\n",
    "\n",
    "\treturn final_conv_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 16, 16, 64], name='tf.compat.v1.shape_119/Shape:0', description=\"created by layer 'tf.compat.v1.shape_119'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(6,), dtype=tf.int32, name=None), inferred_value=[None, 4, 16, 16, 4, 4], name='tf.compat.v1.shape_120/Shape:0', description=\"created by layer 'tf.compat.v1.shape_120'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 16, 16, 64], name='tf.compat.v1.shape_121/Shape:0', description=\"created by layer 'tf.compat.v1.shape_121'\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, 16, 16, 64), dtype=tf.float32, name=None), name='tf.reshape_39/Reshape:0', description=\"created by layer 'tf.reshape_39'\") of unsupported type <class 'keras.engine.keras_tensor.KerasTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-742e56888962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbidir_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbidirectional_conv_lstm_attention_bottleneck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-683a5b389ed7>\u001b[0m in \u001b[0;36mbidirectional_conv_lstm_attention_bottleneck_model\u001b[0;34m(d_model, num_unet_sections, num_layers_attention, convlstm_kernel_size, activation, image_dims, seq_len_prev_and_after, attention_bottleneck_multiple, num_heads, attention_kernel_size, attention_feature_activation, attention_output_activation, unet_upsample_kernel, prediction_activation)\u001b[0m\n\u001b[1;32m     98\u001b[0m                         \u001b[0mfeature_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_feature_activation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0moutput_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_output_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \t\t)(attention_bottleneck)\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mprev_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_bottleneck\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_bottleneck_multiple\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattention_bottleneck_block_height\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mattention_bottleneck_block_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    891\u001b[0m         3, \"Failed to convert %r to tensor: %s\" % (type(value).__name__, e))\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m   raise TypeError(f\"Could not build a TypeSpec for {value} of \"\n\u001b[0m\u001b[1;32m    894\u001b[0m                   f\"unsupported type {type(value)}.\")\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, 16, 16, 64), dtype=tf.float32, name=None), name='tf.reshape_39/Reshape:0', description=\"created by layer 'tf.reshape_39'\") of unsupported type <class 'keras.engine.keras_tensor.KerasTensor'>."
     ]
    }
   ],
   "source": [
    "bidir_model = bidirectional_conv_lstm_attention_bottleneck_model()\n",
    "# TODO: Deal with error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
